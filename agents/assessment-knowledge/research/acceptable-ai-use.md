## **Assessment & Evaluation in Higher Education**

**[ISSN: 0260-2938 (Print) 1469-297X (Online) Journal homepage: www.tandfonline.com/journals/caeh20](https://www.tandfonline.com/journals/caeh20?src=pdf)**

# **‘Where’s the line? It’s an absurd line’: towards** **a framework for acceptable uses of AI in** **assessment**


**Thomas Corbin, Phillip Dawson, Kelli Nicola-Richmond & Helen Partridge**


**To cite this article:** Thomas Corbin, Phillip Dawson, Kelli Nicola-Richmond & Helen Partridge
(2025) ‘Where’s the line? It’s an absurd line’: towards a framework for acceptable uses
of AI in assessment, Assessment & Evaluation in Higher Education, 50:5, 705-717, DOI:
[10.1080/02602938.2025.2456207](https://www.tandfonline.com/action/showCitFormats?doi=10.1080/02602938.2025.2456207)


**To link to this article:** [https://doi.org/10.1080/02602938.2025.2456207](https://doi.org/10.1080/02602938.2025.2456207)


© 2025 The Author(s). Published by Informa
UK Limited, trading as Taylor & Francis
Group


Published online: 24 Jan 2025.


[Submit your article to this journal](https://www.tandfonline.com/action/authorSubmission?journalCode=caeh20&show=instructions&src=pdf)


Article views: 14142


[View related articles](https://www.tandfonline.com/doi/mlt/10.1080/02602938.2025.2456207?src=pdf)


[View Crossmark data](http://crossmark.crossref.org/dialog/?doi=10.1080/02602938.2025.2456207&domain=pdf&date_stamp=24%20Jan%202025)


[Citing articles: 14 View citing articles](https://www.tandfonline.com/doi/citedby/10.1080/02602938.2025.2456207?src=pdf)


Full Terms & Conditions of access and use can be found at
[https://www.tandfonline.com/action/journalInformation?journalCode=caeh20](https://www.tandfonline.com/action/journalInformation?journalCode=caeh20)


Assessment & Evaluation in Higher Education

2025, VOL. 50, NO. 5, 705–717

[https://doi.org/10.1080/02602938.2025.2456207](https://doi.org/10.1080/02602938.2025.2456207)

### **‘Where’s the line? It’s an absurd line’: towards a framework** **for acceptable uses of AI in assessment**


Thomas Corbin [a], Phillip Dawson [b], Kelli Nicola-Richmond [b] and Helen
Partridge [b]


a Centre for Research in Assessment and Digital Learning (CRADLE), Deakin University, Australia; b Deakin
University, Australia;



**ABSTRACT**
As higher education grapples with ensuring assessment validity in an
increasingly AI-populated time, institutions and educators are working to
establish appropriate boundaries for AI use. However, little is known about
how students and teachers conceptualize and experience these boundar­
ies in practice. This study investigates how students and teachers navigate
the line between acceptable and unacceptable AI use in assessment,
drawing on a thematic analysis of qualitative interviews with 19 students
and 12 staff at a large Australian university informed by social boundary
theory. The titular metaphor of ‘drawing a line’ emerged organically from
both students and staff in our interviews, revealing ongoing struggles to
understand and articulate what counts as appropriate. We found that stu­
dents frequently construct their own individually unique and often com­
plex ethical frameworks for AI use. Teachers, meanwhile, report significant
emotional burden and professional uncertainty as they attempt to under­
stand and communicate what is appropriate to their students. Our analy­
sis suggests that assessment policies for AI ought to move beyond simple
prohibitions or permissions and begin to address three critical dimensions:
the feasibility of enforcement, the preservation of authentic learning, and
the emotional wellbeing of teachers and students.


**Introduction**



**KEYWORDS**
Academic integrity;
artificial intelligence;
assessment design;
higher education



Generative Artificial Intelligence (GenAI) tools, such as ChatGPT, allow students to produce out­
puts that may not genuinely reflect their own competencies, understanding, knowledge, and
skills (Dawson et al. 2024). Consequently, the question of how to effectively incorporate or limit
AI use in assessment has become a pressing concern (Kumar et al. 2024). Numerous frameworks
have emerged to help educators adapt to this new AI populated reality, frameworks which aim
to articulate boundaries between acceptable and unacceptable uses of AI (see for example,
Perkins et al. 2024; Davis 2024). Yet despite these efforts, little is known about how educators
and students currently perceive these boundaries, or how these perceptions shape their experi­
ences and decision making. Without this view in mind, it is difficult to know how appropriate any
given boundary actually is.


**CONTACT** Thomas Corbin [t.corbin@deakin.edu.au](mailto:t.corbin@deakin.edu.au)
© 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group

[This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License (http://](http://creativecommons.org/licenses/by-nc-nd/4.0/)
[creativecommons.org/licenses/by-nc-nd/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided](http://creativecommons.org/licenses/by-nc-nd/4.0/)
the original work is properly cited, and is not altered, transformed, or built upon in any way. The terms on which this article has been pub­
lished allow the posting of the Accepted Manuscript in a repository by the author(s) or with their consent.


706 T. CORBIN ET AL.


Understanding acceptable boundaries in the context of AI is a challenge, perhaps far more so
than with previous technologies, because of how deeply the technology integrates into core
academic practices. While earlier challenges to assessment validity, such as essay mills or contract
cheating services, involved clear substitution of student work, GenAI blurs the line between
acceptable assistance and inappropriate delegation at every stage of the assessment process –
from ideation through to final editing (Luo 2024; Corbin et al. 2024). As students and teachers
attempt to navigate these blurred boundaries, they face questions that existing frameworks
struggle to answer for them. When does AI-assisted editing become AI-authored content? At
what point does using AI for brainstorming become outsourced thinking? Such questions reveal
a challenge that requires attention.

In this study, we spoke with 12 educators and 19 students about their views on GenAI and
assessment. Unprompted, many participants asked the question ‘where is the line’ between
acceptable and unacceptable AI use, frequently employing this metaphor explicitly. Their
responses revealed a striking lack of clarity and complexity in their minds, with both students
and staff expressing uncertainty about how to navigate this ambiguity in practice. This uncer­
tainty is not merely an abstract concern; for many, it created a profound sense of unease, even
trauma, as they attempted to reconcile their educational values and priorities with the realities
of AI technology.

These results suggest an urgent need for institutions to engage in meaningful conversations
about AI use in assessment and to provide clear guidance for both assessors and students. This
paper addresses this need by exploring how students and educators conceptualize the boundar­
ies of AI use in assessment and the implications of this ambiguity for teaching and learning.

To achieve the aims of this paper, we first outline our methodological approach. This approach
draws on boundary work and social boundary theory to understand how educational communi­
ties construct and maintain distinctions between legitimate and illegitimate practices. We then
present our findings, examining how both students and teachers attempt to draw workable lines
around AI use through various strategies, including seeking insights from analogous technolo­
gies, developing individual frameworks, and negotiating academic integrity concerns. Our discus­
sion examines how these boundary-drawing efforts reflect deeper tensions in how higher
education maintains its authority and legitimacy in an AI-enabled world. We conclude by propos­
ing a novel Dynamic Educational Boundaries Model that moves beyond simple prohibitions and
permissions to address the complex realities of AI use in contemporary higher education.


**Methodology**


This study adopted a qualitative interview approach to examine how students and educators
view GenAI use in assessments. A qualitative design was chosen for its capacity to capture the
nuanced, context-dependent, and deeply personal insights that underlie these perceptions, which
would be difficult to surface through quantitative methods alone.

The participants comprised 19 students and 12 academic staff members from a large Australian
university, representing diverse disciplines and levels of study. Participants were recruited through
a purposive sampling approach ensuring a diversity of perspectives, with a primary focus on
capturing the distinct experiences and viewpoints of students and faculty members. Given their
differing roles in assessment—students as recipients and faculty as designers and evaluators—we
anticipated that their perspectives on AI’s role in assessment would vary significantly. We also
sought variation across disciplines and, although we did not pre-screen to ensure a variety of
views on AI technology, we included statements on our recruitment materials clarifying that par­
ticipants with any and all levels of experience and familiarity with the technology were welcome.

A semi-structured interview protocol was developed for this study to ensure consistency while
allowing participants the flexibility to elaborate on their experiences. No existing protocols
directly addressed our research focus—AI in assessment from both student and faculty


Assessment & Evaluation in Higher Education 707


perspectives—so the interview questions were designed to capture insights into key concerns
such as fairness, skill development, and acceptable AI use. The guiding questions were informed
by preliminary discussions within the research team, ensuring alignment with the broader con­
ceptual framing of AI as a boundary object in education. The protocol was structured to maintain
coherence across interviews, while the semi-structured approach allowed for follow-up probing
to explore emerging themes. The interview protocol proved effective in eliciting diverse and
reflective responses. Participants provided rich descriptions of their concerns, experiences, and
expectations regarding AI in assessment, and key themes—such as anxieties about fairness, the
perceived role of AI in learning, and uncertainty around institutional policies—emerged consis­
tently across interviews.

Interviews were conducted over Zoom in late 2023 and early 2024. Each interview lasted
between 30–60 min. All interviews were audio-recorded with participant consent and transcribed
verbatim. To promote confidentiality, identifying details were removed from transcripts, and par­
ticipants were assigned pseudonyms. Ethical approval for the study was obtained (HAE-23-057)
and all participants provided informed consent.

The data was analyzed using inductive thematic analysis, following the guidelines of Braun
and Clarke (2006). Initially, the full dataset was coded broadly to sensitise us to the experiences
of staff and students. During this phase, the metaphor of ‘where the line is’ was frequently
observed, with participants expressing uncertainty and concern about the boundaries of AI use.
Based on this insight, re-analysis involved iterative coding to develop sub-themes related to per­
ceptions of appropriate and inappropriate AI use. The analysis was semantic in focus, emphasiz­
ing the explicit language and metaphors participants used to articulate their experiences. The
final themes were reviewed collaboratively by the research team to support consistency and
credibility.

Our analysis was informed by the concept of boundary work (Gieryn 1983, 1999) and social
boundary theory (Lamont and Molnár 2002), which provides a framework for understanding how
groups construct, maintain and negotiate symbolic boundaries, particularly during periods of
social or technological change. This theoretical lens was chosen after our initial data analysis
revealed participants’ persistent use of boundary metaphors (particularly ‘the line’) when discuss­
ing AI use. Social boundary theory helped illuminate how participants worked to construct and
maintain distinctions between legitimate and illegitimate academic practices in the face of tech­
nological disruption. This theoretical framing guided our later analytical phases, helping us
understand participants’ boundary work not just as individual meaning-making but as part of
broader processes of maintaining educational legitimacy and professional identity. The approach
allowed us to examine both the explicit boundaries participants tried to draw around AI use and
the implicit assumptions about educational practice that these boundary-drawing efforts revealed.

Rather than adopting a purely constructivist stance, our analysis recognized that while bound­
aries are socially constructed, they have real consequences for educational practice and emo­
tional wellbeing – what Lamont and Molnár (2002) term the relationship between symbolic and
social boundaries. This theoretical orientation informed our coding process, leading us to pay
particular attention to how participants constructed, justified, and struggled with boundaries
around AI use, while remaining sensitive to the practical and emotional implications of these
boundary negotiations.


**Results**


_**A desire for ‘lines’**_


The metaphor of ‘drawing the line’ emerged organically across our interviews with both students
and staff, appearing spontaneously in over half of all interviews (51.6% overall, 11 of 19 student
and 5 of 12 staff interviews). Both teachers and students invoked this explicitly as well as related


708 T. CORBIN ET AL.


metaphors to express their uncertainty regarding GenAI: ‘There is no very clear line, how should
you deal with this Chat GPT or any of the AI’ one teacher (A03) explained. Others similarly stated,
‘we sort of have to draw the line and think about where the line actually stands’ (A01). The frus­
tration this created was evident: ‘It makes absolutely no sense. Where’s the line? It’s an absurd
line’ (A02).

Students’ responses revealed similar struggles with lines. ‘[GenAI] blurs the line’ (P18) one stu­
dent observed, while others described encountering a ‘murky’ (P02) or ‘grey area’ (P01) when
trying to decide what was appropriate. As one student explained, they were left playing a ‘guess­
ing game’ (P02) to determine the ‘things that are inappropriate’ (P17). In articulating these chal­
lenges, students employed various terms related to ‘lines’, including ‘boundaries’ (P01), ‘guidelines’
(P17), and ‘benchmarks’ (P10). Given this uncertainty about boundaries, many students empha­
sized the need for clear institutional guidance about acceptable AI use in their assessments. One
student, for example, stated:


I think, each university or each professor should be aware that AI is there, it’s free to use, and of course
students will use it. But I would not say like they should discourage them, but in fact, they should be like
they should tell how much we can use it, that would be like setting the limits. (P10)


The impact of this ambiguity extended beyond simple uncertainty about rules. Students
described having to navigate complex decisions about AI use, particularly when trying to under­
stand and accommodate different teacher perspectives:


You have to kinda guess your teacher’s perspective on AI, and then figure out how much to use it or not,
because if there’s someone quite opposed to AI, it’s not worth using it, because you might be marked down,
or […] maybe I can get away with cheating because they’re ignorant. (P02)


Students frequently made comments similar to the above: ‘we have to have some parameters
there about what is considered to be a cheat and what is considered like okay’. (P05) This need
was articulated by some students in terms of the widespread nature of its use: ‘I think it is just
the responsibility of the university may be to address the issue because most of the students will
be using it, but may be using it in a bad way without knowing’. (P14)

Teaching staff expressed parallel concerns, often in the context of the degree of their ability
to provide clear guidance to students. As one teacher explained:


I think once the university has a clear stance, and maybe they do and I just don’t know what it is. Once
they have a very clear stance, I think it’s really important that everybody is, everybody understands what
that is. That academics understand precisely how to escalate, what it is, is something escalatable, and that
we are confident in, I guess, articulating that to students. (A07)


The biggest challenge we saw for teachers in communicating the acceptable line to students
was that they didn’t know it themselves, ‘we’re not sure what what’s OK, what’s not OK’. (A07).


_**Identifying analogous lines**_


Several staff and students offered analogous lines in their reasoning about what was appropriate
use of GenAI in assessment, making comparison to existing technology and practice. One stu­
dent, for example, stated that: ‘It’s a lot like Grammarly, like no one ever says they use Grammarly,
because, like is just seen as like an accepted tool’. (P03) Teachers also sought existing compari­
sons and arrived at Grammarly, ‘if it is helping you to rephrase a bit on your English, I don’t think
it’s a big problem because nowadays you have lots of other tools, like for example I mentioned
the Grammarly subscription’. (A03) Another teacher (A06) similarly stated:


But we, but we’re pretty upfront with them, saying we know you’re going to use it, in fact, you probably
should. […] So, saying, you know, take this if you’re not sure how to word something, put it in the ChatGPT


Assessment & Evaluation in Higher Education 709


and ask it to critique it for you, we encourage it and same with using Grammarly. Like if Grammarly’s got
an idea about what that sentence should look like, look at it and think about it and if you think it works,
go for it, it’s there to help.


For this teacher the line between acceptable and unacceptable was in the same place for
both ChatGPT and Grammarly: use it for feedback but not as a replacement for your own work.
Others made similar analogies with existing technology. For example, when asked about what
they thought of as acceptable use, one student (P09) stated, ‘I think if it’s just things like proof­
reading. Well, that’s okay. Cause I mean, things like spell check pick up similar things like that’.
With the lack of any clear line, it seems clear that both students and staff are reaching for anal­
ogous technologies with their own established lines.

When students and staff referred to established boundaries in their discussions of AI, their
primary concerns often focused on assessment rather than broader questions of syllabus or
instructional design. This emphasis on assessment is perhaps not surprising, as it is the point at
which AI use is most explicitly regulated, where students’ work is formally judged, and where
questions of fairness, integrity, and authorship are most immediate and consequential. This
uncertainty around assessment boundaries left many unsure how to respond.


_**Lines and assessment design**_


The lack of a clear line, for some teachers, meant that they felt unable to respond to the GenAI
challenge, which for them meant appropriately redesigning their assessments. For example, one
teacher told us that, ‘I’ve done absolutely nothing in terms of assessment practises because
I don’t have a clue. (A04)’ Other teachers made similar claims:


I’ve set the tasks, our tasks haven’t changed all too much. […] I suppose we obfuscate the, you know we
sort of ignore the problem because that seems to me to be, that’s [the university’s] sort of policy at the
moment, where if we don’t talk about it, let’s pretend it doesn’t happen and let’s just deal with it case by
case if academic integrity does come up. (A01)


Some students appeared aware of this challenge, stating for example that:


I think it’s very hard to build an assessment, that unless you’re like physically having the kids in the class­
room so you can like, see what they’re doing. It’s very hard to kind of monitor, because, you know, the point
of generative AI is that it will do everything. (P03)


For other teachers though, the inability to be confident about assessment came from a lack
of confidence in understanding the capabilities of the technology, ‘I’m hoping I’m still ahead of
the game with that, but who knows? Who knows?’ (A08) The same teacher discussed possible
options but found them lacking: ‘people have suggested that I do oral exams. Well, I just won’t
get any students if I get oral put in our exams so’. (A08)


_**Designing individual lines**_


One student (P03) told us that ‘it’s really hard, because it can be used like it’s very much up to
you how much you use it’. For similar reasons, students frequently presented their own rationale
for where the line should be:


[T]here’s sort of no, no, clear guidelines on what is appropriate, what is not. It’s just a case of use it judi­
ciously and appropriately, according to what you think should be appropriate. (P01)


Students’ views, however, differed on what they thought was appropriate. For some, it was
based on the concept of understanding; ‘I personally think if you use generative AI to help you


710 T. CORBIN ET AL.


understand the topic, that’s fine, because if you think about it, it’s not really different to like
doing your own research’. (P07) Other students thought similarly, Student 10 stated:


[S]uppose you don’t understand a topic, but Chat GPT gives a very good example, and you now understand
a topic, then it’s beneficial for you.


Others focused more on outputs, saying for example that ‘for me personally, [acceptable
means] not copying directly from Chat GPT’ (P13). Several students focused on the potential neg­
atives to be avoided rather than the gains to be achieved, with one stating that students ‘should
be taught how to use it to not become super dependent on it’. (P16) Some students had more
complex often academic integrity focused rationales behind their use, such as (P17) who stated:


I guess interacting with the Chatbot it does make me think about collusion, because of the way that the
bot speaks as if it is sentient. It speaks like it has sentience. And so, it feels like collusion to talk to it. Which
is not true, because it’s not actually expressing any thoughts or anything like that, or its own opinions but
yeah, but because it’s collusion, I kind of treat how I interact with it as if I was interacting with another
person. So, if I wouldn’t ask another person to write my essay, why would I ask this bot to? […] So yeah,
so I guess that’s kind of how I structured the boundaries I have with it. (P17)


Some teachers thought similarly:


I feel it’s definitely acceptable to use it in some ways, such as getting ideas for structures of assignments.
Maybe rewriting in a nicer way what they’ve said, that’s maybe slightly into the gray area, but still OK. Well,
I guess it’s just a sliding scale, whereas, I’d be uncomfortable if they were getting large swathes of [the
student’s assessment task] written without them really doing reading or considering the material very care­
fully, I’d see that as akin to paying someone to write your essay. You don’t learn the material. You’re going
out into the world as a professional and you don’t really know the material. (A10)


When teachers discussed how they communicate with their students on the topic of appro­
priate AI use, many focused on the final submitted document:


[W]e’re always emphasising that your work needs to be your own […] like if I pop in a question and say,
can you answer this for me, it’ll give me an answer. But if I copy and paste that straight into my work, it’s
now, it’s now a form of plagiarism, because it’s not something I’ve produced, or it’s [at] the very least, it’s
not my own work. And so that I would say is an inappropriate use of chat GPT and we do make that clear.
Just whatever tools you use, it needs to be your work and your work can’t just be control C control V [copy
pasted]. (A06)


When students discussed the advice provided by their teachers, it consequently makes sense
that the most common advice they received was to ‘Make sure you don’t copy paste’. (P01)
However, what this actually meant was challenging to put into practice for students. One student
(P18) stated that they had approached their unit chair on how to use GenAI and they were told
‘you can use it to learn, just don’t copy what you see and just put it there in assessments’. The
student found this advice ‘pretty obvious’ and unhelpful. When they spoke about this advice to
‘a lot of people that are friends … from different faculties’ they found that ‘they were scared of
using GenAI [because] it would just come into plagiarism even if they were just, using it to learn’.
The same student expanded on this point, stating:


[W]hat unit chairs do now is they encourage you to use GenAI, but they won’t tell you how. … that could
mean a lot of different things for different students [and] could really blur the line between academic integ­
rity and actually using it creatively. (P18)


Teachers told us they found it challenging to identify how to draw an acceptable demarcation:
‘I don’t think you can say, to a student, that you can only use ChatGPT for preparation and plan­
ning and sort of getting to the point, and then you have to write your own creative work. It
makes absolute[sic] no sense. Where’s the line? It’s an absurd line’. (A02)


Assessment & Evaluation in Higher Education 711


In figuring this line out, academic integrity concerns clearly drove usage in many cases. One
student (P01) told us:


I know that the university has released a lot of guidelines and a lot of warning signs to students saying that
you have to be really careful using generative AI. So, I’ve mainly used it more for I guess proof reading, or
just sort of getting a head start into an assignment. So, let’s say I’m really stuck on ideas, and I’m not sure
how to approach this paragraph I would put [it] into the Chat GPT and ask it like, how should I sort of explain
this concept of ethics, or maybe medical surgery or something like that, and then it will give me a paragraph
or two, and then I’ll sort of draw inspiration from it and get a couple of words, but I will never sort of use it

[the generated content], because, like, I’m always worried that it will come up as plagiarism. (P01)


Several students commented similarly. For instance (P17) stated, ‘I was very hesitant and
unsure because, first off, I didn’t want to do anything too close to my project because I don’t
know how AI falls, if that falls under plagiarism and cheating’. (P01) similarly stated their use was
‘a bit more cautious, to begin with, just because that’s the whole issue of plagiarism and acad­
emy integrity component associated with it’.

Several students believed there was a real risk of other students intentionally using GenAI for
cheating or dishonest purposes. Some students, however, blamed this risk of plagiarism not on the
technology, but explicitly on the lack of clear lines dictating its use: ‘if students are not taught well
to use them, I think there is a risk of cheating and plagiarisms [sic] ‘. (P01) Others offered similar
sentiments, stating that it is important to clearly set up and state ‘what the university’s boundaries
with AI is. Because I don’t know, I just, I don’t know. I’d hate for someone to get punished for using
it, because they just didn’t know better’. (P17) Teachers had similar views, (A01) for instance stated
that ‘without any guidance, students are going to go about using the tool in the wrong way’.


_**Designing lines is hard work**_


Although it seems likely that questions about the ‘line’ added to student’s workload, at least in
terms of cognitive load, teachers are more obviously burdened by the challenge of finding or
designing an appropriate line. One teacher for example told us:


if you want to ask, you know where the, where’s the black and white response, how we’re actually going to
address this, I have, we have none. There is none. What I would like is some leadership. I had to do it myself.
And although I enjoyed it and I’ve got a bit of notoriety from it, it’s bloody killed me. It really has, like, I’m
absolutely shattered this year. (A08)


The teacher then went on to explain the kinds of ‘black and white responses’ they were
looking for:


I would like the support of the university to say, ‘you don’t have to- Hold on to your exams, you know, you
don’t have to change, you don’t have to rewrite them’. I can’t tell you how many exams I have to write.
I mean, it’s just, it’s beyond belief to think that I’ll have to do that, but yeah. (A08)


Other teachers made similar claims regarding the personal hardships, even trauma, that
attempting to identify the line has created for them:


I’ve had a lot of ethical traumas this year because of, you know, having taught that unit since 2011, from
my perspective, I think there are huge amounts of students that are already using [GenAI]. (A02)


In particular this teacher found the process of marking the most challenging: ‘I’ve been highly
anxious all year marking. I have been second guessing myself’. (A02). Another teacher (A03) sim­
ilarly told us the issue was ‘hugely hurtful to me’. For this teacher, they found that:


I didn’t have anybody to turn to, and I talked to people in the department and there were people who were
often talking about AI, but, you know, it was, it was just like there was no real, forum we could, we could
go to, as staff, that we could air these kind of issues. (A03)


712 T. CORBIN ET AL.


Although obviously assessment change is a demanding task in many ways, this teacher articu­
lated the challenge explicitly in terms of workload, stating that ‘Where’s the workload and all that,
recognition. There isn’t any’. This same teacher had taken long service leave from teaching and,
when time came to return, a lack of answers to these questions forced them to ask themselves:


Do I really want to go back to the tertiary education system where it’s just like a bloody sweatshop?’ Really.
I don’t know, I just, there’s no end to the challenges we’re facing. (A08)


Although some teachers were looking directly at their institution to supply a missing line,
others were dubious that any real solution could be offered universally:


I mean there could be boundaries or some education provided in, say, the academic integrity module, but
then I think each course would have to have their own guidelines based on the how they’re going to actu­
ally use it or how they want their students to use it. Because how some courses may use it could be very
differently[sic] to how another course may want to use it. So, I don’t think a one-size-fits-all approach would
work. (A11)


Other teachers articulated this explicitly as the duty of themselves as a teacher:


it’s my responsibility as an educator to, if my exam can be answered by an AI, if it’s an open book exam,
I have not done my due diligence as an educator to tailor my exam to the unique knowledge of the unit
and to what students are learning. So if my exam, you know if you can get an HD [High Distinction] by
plugging it into Chat GPT, that’s my problem. (A05)


Students were not naive to the challenges of drawing a line. As one student told us, ‘I think
with the guidelines it can be a little bit difficult, because, like each unit is different and it also
comes down to what the student produces by themselves’. (P07). This customisation point came
up repeatedly, including in the context of the ongoing need for institutional policies. (P11) for
instance told us:


There has to be a generic set of guidelines, at least to start off with, and then within specific niches and
specific disciplines, there might be other things like, especially within law, there are a lot of policies that
have to be there are a lot of concerns, for example, privacy concerns as well that come into picture. So for
law, maybe some specific niche disciplines like that, there needs to be additional constraints put in. But
there needs to be generic guidelines that applies across the entire student base.


Student (P05) stated ‘I know that some institutions just have a flat out No AI policy, but I
mean they can’t police that. That’s just that’s just a pipe dream, you know’. This view seems con­
sistent with the way that one student understands their own institutional policy, stating that ‘the
new policy that [university name] have about using it, which is quite sort of a general saying,
‘well, we can’t stop you, but we don’t recommend it’ (P09). Other students shared similar views,
for instance ‘even though there’s policies in the [university name] academic integrity policy about
it, I think it doesn’t really stop someone from doing something if they really wanted to’. (P07)

Teachers similarly shared views, particularly on the success of any strategy aimed at banning
the use of the technology: ‘banning it is silly and not going to work’. (A05) Echoing thoughts of
students above, other teachers commented that ‘we can have lots of nice ideas about, you know,
you can’t use it, you know, AI in this assignment, but we don’t have control over that’. (A10)
Other teachers articulated similar thoughts but based on the logic of the mission of the university:


if you go zero tolerant and say no, we’re not going to adopt it and we’re not going to encourage students
to use it. Then you run the risk of educating a generation of students that doesn’t have the benefit of
accessing the technology, which means those students are going to be at a distinct disadvantage within the
workplace, which means the university brand will suffer. (A04)


For this participant, and many across the data, there was a need to set a line that not just
allowed for the use of AI, but also included instruction and guidance to educate students in ways
appropriate to the world they would enter following graduation.


Assessment & Evaluation in Higher Education 713


**Discussion**


This study set out to explore how educators and students conceptualize and navigate the use of
GenAI in assessment. Our findings reveal that these struggles are not simply about understand­
ing or managing a new technology; they expose deeper questions about how higher education
defines and maintains its boundaries in a technologically evolving landscape.

Boundary work refers to the processes by which individuals and institutions actively create,
negotiate, and sustain distinctions that define legitimacy within a particular space (Langley et al.
2019). Gieryn’s (1983, 1999) early work on boundary work demonstrated how scientists delineate
‘what counts as science’ in response to challenges from pseudoscience, politics, or other compet­
ing domains (Gieryn 1983, 1999). This concept has since been applied within multiple fields to
examine how legitimacy is constructed, co-constructed between individuals and institutions,
managed and contested. These include politics (Solhjell and Klatran 2024), GenAI in workplace
debates (Lombi and Rossero 2024), and educational practices (Ylijoki 2013; Risan 2022; Eastgate
et al. 2023). Applying boundary work to our data allows us to see the ways in which the partic­
ipants in our study engaged in rhetorical, practical, and emotional labour to theorize and define
‘what counts as appropriate’ AI use.

Historically, educators have relied on broadly simple and agreed upon distinctions between
acceptable and unacceptable practices, such as the boundary between authentic student work
and contract cheating (Dawson 2022). Traditional academic boundaries have long offered a recog­
nizable—though sometimes debated—marker for both teachers and students to judge what
counts as acceptable work. For example, the expectation that students produce original work
without external assistance has historically served as a clear boundary, reinforced by plagiarism
policies and assessment designs like closed-book exams. However, AI technologies disrupt these
traditional boundaries by integrating throughout students’ processes, challenging educators’ capac­
ity to distinguish legitimate effort from AI-generated output (Dawson et al. 2024; Jensen et al.
2024). Our findings show that educators are deeply engaged in boundary work as they navigate
the new ambiguities of GenAI in assessment. One participant described the emotional toll of this
disruption: ‘I’ve been highly anxious all year marking. I have been second guessing myself’ (A02).
This anxiety reflects more than procedural uncertainty; it reveals a crisis in educators’ ability to
perform boundary work in a context where previous boundaries collapse under technological
change. Educators’ emotional responses—feeling ‘shattered’ and experiencing ‘ethical traumas’—
highlight the stakes of boundary work in this context. Their labour involves not only enforcing
institutional rules but also maintaining their professional identity as arbiters of academic legitimacy.

Students in our study also engaged in boundary work, albeit with different challenges. Students
often constructed their own frameworks to determine appropriate AI use, even when provided
with alternative – even conflicting – views of from their teachers. Many relied on analogies to
existing technologies, such as Grammarly or spell-checkers, to justify their practices. As one stu­
dent explained, ‘I personally think if you use generative AI to help you understand the topic, that’s
fine, because it’s not really different to like doing your own research’ (P07). These analogies reflect
an attempt to normalize AI use within familiar boundaries, a rhetorical strategy often employed
in boundary work to reconcile new practices with established norms (Lamont and Molnár, 2002).
Similar to other professions, where practitioners engage in boundary work to navigate new norms
in response to emergent technologies (e.g. Christensen and Treas 2024), students’ boundary work
illustrates the tensions between personal judgment and collective expectations.

More broadly, our findings highlight that students are currently engaging in frequent, individ­
ual boundary work as they make case-by-case decisions about AI use in their assignments. As
one student stated, ‘You have to kinda guess your teacher’s perspective on AI, and then figure
out how much to use it or not’ (P02). This process places a significant cognitive and emotional
burden on students as the question of ‘where the line is’ becomes a high-stakes decision, shaped
not only by institutional ambiguity but also by ethical reasoning and risk assessment. Critically,


714 T. CORBIN ET AL.


it seems necessary to conclude that this decision is at least often externalized onto students by
the structure of assessments themselves. That is to say, due to the nature of the ambiguity it is
ultimately up to students to make the decision, often a decision with significant educational
outcomes, of if they should or should not use AI.

In order to remove this heavy decision from the shoulders of students, boundary work theory
makes the perhaps unexpected suggestion that we should not understand the main challenge
of AI as one of clarifying boundaries to students and then enforcing these boundaries. This is
unexpected because it seems to fly in the face of both intuition and the current tide of reactions
to GenAI in higher education, where the focus is often on communicating clear rules to students
regarding what is allowed and what is not. Instead, boundary work suggests removing the need
for students to engage in boundary work altogether.

Boundary work suggests that, as far as is possible, assessment design should eliminate GenAI
ambiguity by embedding clear, actionable expectations for AI use into the tasks themselves.
When assessments are designed with this in mind, the burden of making these decisions no
longer falls on students. This is likely to not only reduce cognitive load and anxiety but also help
ensure that decisions about AI use are pedagogically sound, supporting the integrity of learning
outcomes rather than leaving them to individual student interpretation. However, although this
could potentially improve the situation for students, it is less obvious what would improve the
situation for teachers. This leads us to suggest what we term the Dynamic Educational Boundaries
Model (DEBM), an initial framework emerging from our recognition that the challenge of AI in
education is fundamentally a challenge of boundary work.

Our findings reveal that educators and students are not only navigating blurred lines between
legitimate and illegitimate uses of AI but are also grappling with the emotional and cognitive
demands of constantly interpreting and enforcing these boundaries. The DEBM provides both a
broad diagnostic tool and an applied framework for developing AI-use boundaries in higher edu­
cation, offering a way to conceptualize and address these challenges through three intercon­
nected dimensions: Assessment-Embedded Boundaries, Contextual Flexibility, and Emotional and
Cognitive Support (see Figure 1).

The framework begins with _Assessment-Embedded Boundaries_, which directly respond to the
cognitive load and ambiguity that students and educators currently face. In particular, this ele­
ment of the framework aims to clarify lines to students. Rather than relying on rules or


**Figure 1.** The dynamic educational boundaries model.


Assessment & Evaluation in Higher Education 715


guidelines, this dimension emphasizes the design of assessments where boundaries are clear and
inherent to the task itself. For example, choosing an oral exam over a written one inherently
establishes that oral communication, rather than written fluency, is the skill being assessed. By
embedding boundaries into the task structure, this approach eliminates the need for ad hoc
decision-making, allowing students to focus on demonstrating their learning in ways that align
with the goals of the assessment. Our data shows that students are drawing their own lines
which clearly is problematic for many reasons, not least for assessment validity, thus the first
point of our framework aims to eliminate this challenge. In other words, this element of the
framework relies on the ostensibly simple point that students do not need to create their own
lines when the lines are built into the task themselves. To achieve this, the DEBM frames the
concepts of ‘allowing’ and ‘prohibiting’ AI use as matters of structural design rather than external
rule enforcement. To ‘allow’ AI use is to integrate it purposefully into the learning process, making
its use both clear and valuable for achieving educational outcomes. Conversely, to ‘prohibit’ AI
use is to structure assessments where inappropriate AI reliance undermines the purpose of the
task, such as within oral defences. These structural changes inherently guide behaviour, reducing
the need for explicit rules and fostering clarity for both educators and students.

Contextual Flexibility highlights that what counts as appropriate AI use is not universal but
varies across disciplines, assessment types, and learning objectives. The DEBM challenges the
assumption that AI-use boundaries can be set through generalized institutional policies, instead
emphasizing that appropriate uses must be determined at the level of specific assessments and
disciplinary expectations.

Educators in our study consistently pointed to the tension between disciplinary norms and
the need for clearer AI-use guidelines. One noted that ‘each course would have to have their own
guidelines based on how they’re going to actually use it’ (A11). For example, while AI-assisted
data analysis might align with professional practices in business and engineering, it could con­
tradict core learning outcomes in statistics courses where manual calculation is central. These
variations mean that what constitutes appropriate AI use cannot be meaningfully determined
outside of its disciplinary and assessment context. By clarifying where and why AI-use boundaries
remain uncertain, Contextual Flexibility helps locate the areas requiring further intervention. This
diagnostic function ensures that efforts to structure AI-use boundaries (Assessment-Embedded
Boundaries) and support educators (Emotional and Cognitive Support, discussed below) are tar­
geted at the actual sources of difficulty rather than assumed challenges.

Finally, _Emotional and Cognitive Support_ addresses the toll of boundary work on educators,
who often bear the burden of enforcing unclear or inconsistent guidelines. The DEBM highlights
the importance of institutional support mechanisms, such as professional development work­
shops, shared repositories of assessment examples, and peer mentoring programs. These resources
not only equip educators with the tools to integrate AI effectively but also foster a sense of
collaboration and shared responsibility. By reducing the emotional strain of navigating ambigu­
ous boundaries, this dimension enables educators to focus on fostering learning rather than
policing AI use.

The DEBM is not intended as a final, complete, or static solution but rather as an initial, broad
and yet dynamic approach to understanding and addressing the challenges of AI use in educa­
tion. Grounded in boundary work theory, it seeks to balance clarity, adaptability, and support,
enabling educational institutions to navigate the complexities of integrating AI tools in ways that
uphold academic integrity and promote meaningful learning.


**Conclusion**


This study reveals how the seemingly simple question of ‘where’s the line?’ in AI use masks profound
challenges to educational practice and identity. Through the lens of social boundary theory, we can
understand participants’ persistent search for clear boundaries not merely as a practical concern, but


716 T. CORBIN ET AL.


as fundamental work to maintain educational legitimacy in the face of technological disruption. The
metaphor of ‘the line’, emerging organically from our participants, captures both the appeal and the
impossibility of clear demarcations in an increasingly AI-mediated educational landscape.

The concept of boundaries is particularly valuable because it illuminates the ways in which
seemingly individual concerns, like a teacher’s uncertainty about grading or a student’s anxiety over
AI use, are connected to larger institutional and cultural processes. Educators’ emotional responses—
describing themselves as ‘shattered’ or grappling with ‘ethical traumas’—speak to how AI unsettles
established norms and professional identities. These responses are not just reactions to a new tool
but are boundary-making processes that attempt to reconcile conflicting expectations of fairness,
rigor, and technological progress. Similarly, when students create analogies to tools like Grammarly
or engage in moral reasoning about AI’s role, they reveal how the absence of clear institutional
guidance forces them into sophisticated but unsupported forms of boundary work. This labor, both
cognitive and emotional, underscores the inadequacy of current structures to accommodate the
realities of AI-mediated education. By framing the challenges of AI in education as boundary work,
this study contributes a critical lens to the scholarship on generative AI and assessment. It shifts
the focus from narrow debates about permissibility or prohibition to the broader processes of
negotiation and redefinition that underpin educational legitimacy.

The emotional and professional toll of uncertain boundaries suggests that what appears as
simple anxiety about new technology actually reflects a deeper crisis in educational identity and
authority. When teachers report feeling ‘shattered’ or experiencing ‘ethical traumas’, they are
expressing how AI disrupts not just assessment practices but fundamental understandings of
educational legitimacy. Similarly, when students construct elaborate personal frameworks for AI
use, they engage in sophisticated boundary work that reveals both the inadequacy of simple
guidelines and the deep connection between boundaries and learning itself.

The disciplinary variations in how participants construct and maintain boundaries reveals the
deeply contextual nature of educational legitimacy. What counts as appropriate AI use varies not
just between disciplines but between different types of learning tasks and outcomes. This sug­
gests that any framework for managing AI use must be dynamic enough to accommodate these
variations while still maintaining meaningful educational standards.

The Dynamic Educational Boundaries Model we propose offers one way forward, suggesting
that educational boundaries should be understood not as fixed lines but as negotiated spaces
that evolve with technological capability and educational need. This framework has immediate
practical implications while also pointing to deeper questions about the nature of learning and
assessment in an AI-enabled world. Most importantly, it suggests that supporting the emotional
and professional wellbeing of teachers and students is not peripheral but central to maintaining
educational integrity in the face of technological change.

Boundary work is difficult, not least in its insistence that no universal solution will be appro­
priate. Future work will and should continue to examine how educational communities negotiate
these boundaries as AI capabilities and higher education solutions evolve. However, what emerges
from our analysis is that the challenge of AI in education, whilst no doubt a challenge of bound­
aries, goes beyond simple questions of permissibility or prohibition.


**Disclosure statement**


No potential conflict of interest was reported by the author(s).


**Notes on contributors**


_**Thomas Corbin**_ is a Research Fellow in the Centre for Research in Assessment and Digital Learning (CRADLE) at
Deakin University. His research focuses on GenAI, assessment, and the philosophy of education.


Assessment & Evaluation in Higher Education 717


_**Phillip Dawson**_ is a professor and Co-Director of the Centre for Research in Assessment and Digital Learning
(CRADLE) at Deakin University. His research focuses on assessment, feedback, and cheating.


_**Kelli Nicola-Richmond**_ is an associate professor and Associate Head of School, Teaching and Learning of the School
of Health & Social Development at Deakin University. Her research focuses on evaluative judgement, exams and
cheating, Long COVID and student underperformance and failure in clinical placement.


_**Helen Partridge**_ is a professor and Pro Vice-Chancellor, Teaching and Learning at Deakin University. Her research
focuses on the interplay between information, technology and learning.


**ORCID**


Thomas Corbin [http://orcid.org/0000-0001-8750-5711](http://orcid.org/0000-0001-8750-5711)
Phillip Dawson [http://orcid.org/0000-0002-4513-8287](http://orcid.org/0000-0002-4513-8287)
Kelli Nicola-Richmond [http://orcid.org/0000-0003-4874-5055](http://orcid.org/0000-0003-4874-5055)
Helen Partridge [http://orcid.org/0000-0002-8365-870X](http://orcid.org/0000-0002-8365-870X)


**References**


Braun, V., and V. Clarke. 2006. “Using Thematic Analysis in Psychology.” _Qualitative Research in Psychology_ 3 (2): 77–101.

[doi:10.1191/1478088706qp063oa.](https://doi.org/10.1191/1478088706qp063oa)
Christensen, M., and J. Treas. 2024. “Digital Boundary Work and Work-to-Family Spillover in Europe: Examining the

Role of Digital Skills.” _Community, Work & Family_ [27 (5): 649–672. doi:10.1080/13668803.2024.2412700.](https://doi.org/10.1080/13668803.2024.2412700)
Corbin, T., Y. Liang, M. Bearman, T. Fawns, G. Flenady, P. Formosa, L. McKnight, J. Reynolds, and J. Walton. 2024.

“Reading at University in the Time of GenAI.” _Learning Letters_ [3: 35. doi:10.59453/ll.v3.35.](https://doi.org/10.59453/ll.v3.35)
Davis, M. 2024. “Supporting Inclusion in Academic Integrity in the Age of GenAI.” _Using Generative AI Effectively in_

_Higher Education: Sustainable and Ethical Practices for Learning, Teaching and Assessment_ . United Kingdom: Taylor
& Francis.

Dawson, P. 2022. “Inclusion, Cheating, and Academic Integrity: Validity as a Goal and a Mediating Concept.” In

_Assessment for Inclusion in Higher Education_, 110–119. New York: Routledge.
Dawson, P., M. Bearman, M. Dollinger, and D. Boud. 2024. “Validity Matters More than Cheating.” _Assessment &_

_Evaluation in Higher Education_ [49 (7): 1005–1016. doi:10.1080/02602938.2024.2386662.](https://doi.org/10.1080/02602938.2024.2386662)
Eastgate, L., P. A. Creed, M. Hood, and A. Bialocerkowski. 2023. “It Takes Work: How University Students Manage Role

Boundaries When the Future is Calling.” _Research in Higher Education_ [64 (7): 1071–1088. doi:10.1007/s11162-023-09736-9.](https://doi.org/10.1007/s11162-023-09736-9)
Gieryn, T. F. 1983. “Boundary-Work and the Demarcation of Science from Non-Science: Strains and Interests in

Professional Ideologies of Scientists.” _American Sociological Review_ [48 (6): 781–795. doi:10.2307/2095325.](https://doi.org/10.2307/2095325)
Gieryn, T. F. 1999. _Cultural Boundaries of Science: Credibility on the Line_ . Chicago: University of Chicago Press.
Jensen, L. X., A. Buhl, A. Sharma, and M. Bearman. 2024. “Generative AI and Higher Education: A Review of Claims

from the First Months of ChatGPT.” _Higher Education:_ [1–17. doi:10.1007/s10734-024-01265-3.](https://doi.org/10.1007/s10734-024-01265-3)
Kumar, R., S. E. Eaton, M. Mindzak, and R. Morrison. 2024. “Academic Integrity and Artificial Intelligence: An

Overview.” In S. E. Eaton, editor. _Second Handbook of Academic Integrity_, 1583–1596. New York: Springer.
[doi:10.1007/978-3-031-54144-5_153.](https://doi.org/10.1007/978-3-031-54144-5_153)
Lamont, M., and V. Molnár. 2002. “The Study of Boundaries in the Social Sciences.” _Annual Review of Sociology_ 28 (1):

[167–195. doi:10.1146/annurev.soc.28.110601.141107.](https://doi.org/10.1146/annurev.soc.28.110601.141107)

Langley, A., K. Lindberg, B. E. Mørk, D. Nicolini, E. Raviola, and L. Walter. 2019. “Boundary Work among Groups,

Occupations, and Organizations: From Cartography to Process.” _Academy of Management Annals_ 13 (2): 704–736.
[doi:10.5465/annals.2017.0089.](https://doi.org/10.5465/annals.2017.0089)

Lombi, L., and E. Rossero. 2024. “How Artificial Intelligence is Reshaping the Autonomy and Boundary Work of

Radiologists. A Qualitative Study.” _Sociology of Health & Illness_ [46 (2): 200–218. doi:10.1111/1467-9566.13702.](https://doi.org/10.1111/1467-9566.13702)
Luo, J. 2024. “A Critical Review of GenAI Policies in Higher Education Assessment: A Call to Reconsider the

“Originality” of Students’ Work.” _Assessment & Evaluation in Higher Education_ 49(5): 1–14.
Perkins, M., L. Furze, J. Roe, and J. MacVaugh. 2024. “The Artificial Intelligence Assessment Scale (AIAS): A Framework

for Ethical Integration of Generative AI in Educational Assessment.” _Journal of University Teaching and Learning_
_Practice_ [21 (6): 1–18. doi:10.53761/q3azde36.](https://doi.org/10.53761/q3azde36)
Risan, M. 2022. “Negotiating Professional Expertise: Hybrid Educators’ Boundary Work in the Context of Higher

Education-Based Teacher Education.” _Teaching and Teacher Education_ [109: 103559. doi:10.1016/j.tate.2021.103559.](https://doi.org/10.1016/j.tate.2021.103559)
Solhjell, R., and H. K. Klatran. 2024. “The Politics of Policing Hate: Boundary Work, Social Inequalities, and Legitimacy.”

_Social & Legal Studies_ [33 (4): 620–641. doi:10.1177/09646639231201912.](https://doi.org/10.1177/09646639231201912)
Ylijoki, O. H. 2013. “Boundary-Work between Work and Life in the High-Speed University.” _Studies in Higher Education_

[38 (2): 242–255. doi:10.1080/03075079.2011.577524.](https://doi.org/10.1080/03075079.2011.577524)


