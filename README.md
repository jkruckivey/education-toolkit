# Education Toolkit - Claude Code Plugin

**Comprehensive toolkit for educational developers** with 12 specialized agents, 10 slash commands, and automatic code review. Includes cutting-edge assessment methodologies: PAIRR (Peer and AI Review + Reflection), AI Roleplay exercises, diagnostic rubrics, multi-perspective peer design review simulation, and fullstack code review with automatic quality checks.

> **âœ¨ NEW in v2.5.0**: Automatic code review hook for FastAPI/Python and React/JSX with PostToolUse triggers

## Installation

```bash
/plugin marketplace add jameskruck/education-toolkit
/plugin install education-toolkit
```

**Migrating from claude-subagents NPM package?** This plugin replaces `@jameskruck/claude-subagents` with enhanced functionality (slash commands + same agents).

## What's Included

### ðŸ¤– 12 Specialized Agents

1. **accessibility-auditor** - WCAG 2.2 AA compliance checking with WebFetch capability
2. **assessment-designer** - Comprehensive assessment design with 464 KB bundled knowledge base. **NEW**: PAIRR methodology, AI Roleplay exercise design, Three-Tier AI permission model
3. **backend-reviewer** - **NEW**: FastAPI/Python code review for security, error handling, API design, and performance
4. **branding-checker** - Canvas LMS and Uplimit platform branding validation
5. **consistency-checker** - Cross-module terminology, concept threading, outcome alignment
6. **frontend-reviewer** - **NEW**: React/JSX code review for accessibility (WCAG 2.2 AA), UX patterns, and performance
7. **peer-review-simulator** - **NEW**: Multi-perspective design review with 6 ID specialists (Emma-Content, Marcus-Accessibility, Priya-Visual, James-Technical, Sarah-Pedagogy, Alex-UX)
8. **rubric-generator** - QM-aligned rubric generation. **NEW**: Diagnostic rubrics (3-level), PAIRR bonus rubrics, AI feedback prompts
9. **student-journey-simulator** - 4-persona course experience testing (Sarah, Marcus, Priya, Alex)
10. **udl-content-generator** - Transform content into multimodal formats (audio, visual, interactive)
11. **uplimit-storyboard-builder** - Complete copy-paste-ready implementation guides for Uplimit courses
12. **widget-tester** - 3-persona widget testing (Sarah, James, Maria) for UX validation

### âš¡ 10 Quick Slash Commands

1. **/audit-module** - Audit module for WCAG 2.2 AA accessibility compliance
2. **/build-storyboard** - Build comprehensive Uplimit storyboard with copy-paste-ready content
3. **/check-branding** - Validate Canvas LMS or Uplimit platform branding consistency
4. **/check-consistency** - Check consistency across modules (terminology, threading, narrative)
5. **/design-assessment** - Design comprehensive assessment with AI integration and UDL/QM compliance
6. **/generate-rubric** - Generate QM-aligned assessment rubric (quick rubric-only tasks)
7. **/peer-review** - **NEW**: Simulate design review panel with 6 ID specialists
8. **/review-content** - Quick content review for educational quality
9. **/simulate-journey** - Simulate student journey through modules with 4 personas
10. **/test-widget** - Test interactive widget with 3 student personas

### ðŸ“š Bundled Knowledge Base (464 KB)

The **assessment-designer** agent includes:
- **4 Framework Guides**: UDL, Quality Matters, Inclusive Teaching, Assessment Templates
- **5 Research Papers**: AI Assessment Framework, Acceptable AI Use, Alternative Assessments, Academic Integrity, GenAI in Higher Ed
- **AI Assessment Principles**: Three-Tier Permission Model, Social Boundary Theory, AI-Resistant Design

### âœ¨ NEW: Evidence-Based Assessment Methodologies

#### PAIRR (Peer and AI Review + Reflection)
**Research-backed dual-feedback methodology** where students receive feedback from both a peer and AI, then critically compare both sources through structured reflection.

**Benefits:**
- Develops critical AI literacy (students evaluate AI output quality)
- Multiple perspectives improve writing quality
- Students maintain writerly agency (choose which feedback to apply)
- Prepares for AI-integrated workplaces

**Implementation:**
1. Student submits 80% draft
2. Gets peer review (rubric-aligned) + AI feedback (same rubric)
3. Completes comparative reflection (150-200 words)
4. Revises based on critical evaluation
5. Post-revision reflection on which feedback influenced most

**Ask agents:** "Design a PAIRR workflow for my reflection memo" or "Generate a PAIRR rubric with bonus points"

#### AI Roleplay Exercises
**Conversational assessments** where students engage with AI character roleplaying as stakeholder, expert, or decision-maker.

**Three Types:**
1. **Diagnostic** (pre-learning): Reveals gaps before content delivery
2. **Formative** (during learning): Practice before summative assessment
3. **Summative** (after learning): Demonstrates mastery through conversation

**Benefits:**
- Tests application & synthesis (higher Bloom's levels)
- Authentic communication practice
- Immediate conversational feedback
- Accessibility: Oral assessment alternative

**Ask agents:** "Design an AI Roleplay for practicing [skill]" or "Create a diagnostic roleplay exercise"

#### Diagnostic Rubrics (3-Level)
**Formative pre-learning rubrics** with Beginning â†’ Developing â†’ Proficient levels (no "Exemplary" since mastery not expected).

**Key Features:**
- Typical student responses at each level
- Support flags (when to provide scaffolding)
- Non-evaluative tone ("This helps you identify what to focus on")
- 2-3 minute grading time per student

**Ask agents:** "Generate a diagnostic rubric for pre-assessment" or "Create a 3-level formative rubric"

### ðŸš€ NEW: Executable Skills (Python Automation)

**Agents now invoke executable Python skills for automated template generation and validation.**

#### assessment-template-generator
**Automated PAIRR, AI Roleplay, and Diagnostic Rubric generation**
- Generates complete PAIRR assignment templates (rubric + AI prompt + reflections + faculty guide)
- Creates Uplimit AI Roleplay configurations (diagnostic/formative/summative types)
- Builds 3-level diagnostic rubrics with support flags
- **Agents invoke automatically** when you request these assessment types

**Example**: "Create a PAIRR assignment for my marketing memo" â†’ Agent runs Python script â†’ Complete template generated

#### accessibility-audit-tools
**Automated WCAG 2.2 AA compliance checking**
- Color contrast validator (calculates ratios, suggests fix colors)
- Alt text checker (presence + quality patterns)
- Heading hierarchy validator
- ARIA compliance scanner
- **Agents invoke for first-pass automation**, then manual review

**Example**: "Check accessibility compliance" â†’ Agent runs automated tests â†’ Detailed report with line numbers and fixes

#### qm-validator
**Quality Matters standards validation**
- Outcome-criteria alignment checker (detects orphaned criteria)
- Rubric math validator (point totals, distribution balance)
- Measurable language analyzer (Bloom's taxonomy verbs)
- **Agents invoke proactively** after generating rubrics

**Example**: Agent generates rubric â†’ Auto-validates QM compliance â†’ Fixes issues before presenting to you

**Why Skills Matter**: These executable tools save 10-15 minutes per assessment by automating repetitive structure generation and validation, letting agents focus on customization and design advice.

**Distribution Options:**
- **Bundled** (default): Skills included with plugin, zero config
- **Standalone**: Download ZIPs from [releases](https://github.com/jkruckivey/education-toolkit/releases) for use with other plugins

ðŸ“– **[Skills Installation Guide](SKILLS-INSTALLATION.md)** - Detailed setup instructions, requirements, and troubleshooting

### ðŸ”„ NEW: Automatic Quality Enforcement (Hooks)

**Built-in automation that runs quality checks automatically - zero API tokens, zero manual effort.**

This plugin includes 5 automatic hooks that enforce quality standards in real-time:

#### Hook 1: Smart Content Validator (PostToolUse)
**Runs after every file edit** - Instant feedback on quality issues

âœ“ **HTML files**: WCAG 2.2 AA checks (contrast, alt text, headings, buttons, tables)
âœ“ **Markdown files**: Learning outcomes (Bloom's verbs), rubric math, PAIRR components, WCAG version
âœ“ **Storyboards**: Colored emoji detection, terminology consistency

**Example output:**
```
ðŸ“‹ Auto-validation found 2 issue(s):
  âš ï¸  Found 3 instance(s) of vague learning outcome language
     Avoid: understand, know, learn about
     Use: explain, identify, analyze, evaluate

  âš ï¸  Missing alt text on 1 image(s)
     All images need alt="description" or alt="" (if decorative)
```

**Token savings**: ~10,000 tokens per validation Ã— 20 edits/day = 200,000 tokens/day saved

#### Hook 2: Educational Context Loader (SessionStart)
**Runs once at session start** - Sets educational standards context automatically

âœ“ Loads WCAG 2.2 AA, QM 6th Edition, Bloom's Taxonomy, UDL standards
âœ“ Displays available quick commands (/design-assessment, /peer-review, etc.)
âœ“ Shows course-specific config (if `.education-toolkit-config.json` exists)
âœ“ Reminds you of automatic quality checks active

**No more forgetting** which WCAG version to use or which commands are available.

#### Hook 3: Protected Content Guardian (PreToolUse)
**Runs before editing published content** - Prevents accidental changes to student-facing materials

âœ“ Blocks edits to `published/`, `production/`, `final/`, `student-facing/`, `graded/` paths
âœ“ Shows warning with risks (fairness, academic integrity, student trust)
âœ“ Recommends creating draft version first
âœ“ Requires explicit approval to proceed

**Protects against**: Changing rubrics after students submit, modifying published assessments mid-term

#### Hook 4: Storyboard Auto-Formatter (PostToolUse)
**Runs after editing storyboards** - Ensures consistent platform conventions

âœ“ Converts colored emoji â†’ black symbols (ðŸ”´â†’â¬¤ ðŸŸ¡â†’â— ðŸŸ¢â†’â—‹)
âœ“ Standardizes priority badges, element headings, MLO references
âœ“ Fixes table spacing, heading spacing, time estimates
âœ“ Removes trailing spaces, normalizes blank lines

**Example output:**
```
âœ¨ Auto-formatted storyboard:
  â€¢ Converted 8Ã— ðŸ”´ â†’ â¬¤
  â€¢ Converted 5Ã— ðŸ“º â†’ â–¶
  â€¢ Standardized element heading format (dash â†’ colon)
  â€¢ Fixed table spacing
```

#### Hook 5: Fullstack Code Reviewer (PostToolUse)
**Runs after editing Python or React code** - Automatic code review for educational technology projects

âœ“ **Backend (Python/FastAPI)**: Security, input validation, error handling, API design, async/await
âœ“ **Frontend (React/JSX)**: Accessibility (WCAG 2.2 AA), state management, UX patterns, performance
âœ“ **Smart routing**: Detects file type â†’ launches specialized agent (backend-reviewer or frontend-reviewer)
âœ“ **Skips non-code**: Config files, package.json, small formatting changes

**Review criteria:**
- **Security**: Authentication, input sanitization, secret management
- **Accessibility**: ARIA labels, semantic HTML, keyboard navigation, color contrast
- **Best practices**: React hooks, error handling, loading states, user feedback
- **Performance**: Database queries, re-renders, bundle size

**Example output:**
```
ðŸ“ Frontend Review: App.jsx:236-267

âœ… STRENGTHS:
  â€¢ Line 236-248: Excellent assessment-result pattern
  â€¢ Line 245-246: Smart use of _next_content to preload

âš ï¸ SUGGESTIONS:
  â€¢ Line 194-206: getCalibrationMessage could be memoized
  â€¢ Line 249: Consider functional update: setContentIndex(i => i + 1)

ðŸ”´ CRITICAL ISSUES:
  â€¢ None found
```

**When to use manually**: Invoke `backend-reviewer` or `frontend-reviewer` agents directly for comprehensive file review without waiting for edits

#### Performance & Cost

| Metric | Value |
|--------|-------|
| **Execution time** | 1-3 seconds (runs locally) |
| **API tokens used** | 0 tokens (pure bash/Python automation) |
| **Token savings** | ~200,000+ tokens/day for active users |
| **Accuracy** | 100% consistent (deterministic, not LLM-based) |

#### Configuration (Optional)

Create `.education-toolkit-config.json` in your project root for custom settings:

```json
{
  "courseName": "Business of Sports Marketing",
  "institution": "Ivey Business School",
  "platform": "Uplimit",
  "conventions": {
    "learningOutcomePrefix": "MLO",
    "wcagVersion": "2.2"
  },
  "protectedPaths": [
    "published/*",
    "custom/protected/path/*"
  ]
}
```

See `.education-toolkit-config.example.json` for full configuration options.

**Hooks are always active when plugin is enabled** - no setup required. Disable individual hooks by editing `hooks/hooks.json` if needed.

## Usage Examples

### Quick Accessibility Check
```bash
/audit-module modules/week1
```

### Test a Widget
```bash
/test-widget widgets/revenue-empire-builder.html
```

### Review Content Quality
```bash
/review-content modules/week2/outline.html
```

### Generate Assessment Rubric
```bash
/generate-rubric for Week 1 case analysis
```

### Simulate Student Experience
```bash
/simulate-journey modules/week1 modules/week2
```

### Check Course Consistency
```bash
/check-consistency week1-5
```

### Peer Design Review
```bash
/peer-review Week 1
```

## Natural Language Invocation

After installation, you can also invoke agents with natural language:

- "Audit modules/module-1/index.html for accessibility"
- "Test the fan engagement lab widget"
- "Check if this matches Canvas LMS branding"
- "Generate a rubric for this reflection memo"
- "Simulate a student going through Week 1"
- "Check terminology consistency across all modules"

## Use Cases

### Course Development Teams
- Standardize QA workflows across team members
- Ensure consistent accessibility and branding compliance
- Validate pedagogical quality before publishing

### Solo Course Developers
- Get instant feedback on content quality
- Test widgets with multiple student perspectives
- Generate professional rubrics quickly

### Instructional Designers
- Audit courses for WCAG 2.2 AA compliance
- Validate UDL principles implementation
- Check Quality Matters standards alignment

### Educational Technologists
- Test interactive learning tools thoroughly
- Ensure platform branding consistency
- Validate assessment design best practices

## Agent Details

### accessibility-auditor
**Tools**: Read, Glob, Grep, WebFetch
**Speed**: 2-5 minutes (Sonnet)
**Best for**: WCAG 2.2 AA audits, Canvas LMS content, HTML/CSS validation

### widget-tester
**Tools**: Read, Glob
**Speed**: 2-5 minutes (Sonnet)
**Best for**: Interactive widgets, simulations, learning games
**Personas**:
- Sarah (Quick Learner) - tech-savvy, skims instructions
- James (Methodical Analyst) - tests edge cases, explores deeply
- Maria (Struggling Student) - needs clear guidance, frustrated easily

### branding-checker
**Tools**: Read, Glob
**Speed**: 2-5 minutes (Sonnet)
**Best for**: Canvas LMS courses, Uplimit platform content
**Detects**: Uplimit Geist font, neutral grays, NO colored labels/badges

### rubric-generator
**Tools**: Read, Glob, Grep
**Speed**: 2-5 minutes (Sonnet)
**Best for**: QUICK rubric-only tasks
**Note**: For comprehensive assessment design, use assessment-designer instead

### student-journey-simulator
**Tools**: Read, Glob, Grep
**Speed**: 5-8 minutes (Opus)
**Best for**: Multi-week course experience testing
**Personas**:
- Sarah (Visual Learner) - needs diagrams, struggles with text
- Marcus (Analytical Thinker) - wants data and sources
- Priya (Collaborative Leader) - thrives in group work
- Alex (Time-Constrained Professional) - needs efficiency

### consistency-checker
**Tools**: Read, Glob, Grep
**Speed**: 5-8 minutes (Opus)
**Best for**: Cross-module narrative flow, concept threading, terminology audits

### peer-review-simulator
**Tools**: Read, Glob, Grep
**Speed**: 8-12 minutes (Opus)
**Best for**: Comprehensive multi-perspective design review before launch
**Review Panel**:
- Emma (Content & Writing) - Grammar, clarity, inclusive language
- Marcus (Accessibility & Inclusion) - WCAG 2.2 AA, UDL, cultural sensitivity
- Priya (Visual Design & UI) - Typography, layout, visual consistency
- James (Technical & Functionality) - Browser compatibility, performance, security
- Sarah (Pedagogical Design) - Learning alignment, Bloom's accuracy, scaffolding
- Alex (UX & Navigation) - Information architecture, wayfinding, usability
**Output**: Cross-reviewer themes prioritized, individual specialist feedback, fix time estimates

### assessment-designer
**Tools**: Read, Glob, Grep, WebFetch
**Speed**: 3-6 minutes (Sonnet)
**Best for**: Strategic assessment design, AI integration, UDL/QM compliance
**Knowledge Base**: 464 KB (9 files) - frameworks and AI assessment research

## Contributing

Found a bug or have a feature request? Open an issue at:
https://github.com/jameskruck/education-toolkit/issues

## License

MIT License - See LICENSE file for details

## Author

**James Kruck**
Email: jkruck@ivey.ca
GitHub: [@jameskruck](https://github.com/jameskruck)

## Version History

- **2.4.2** (2025-10-21): Fixed peer-review-simulator to distinguish storyboards vs live content
  - Added critical content type detection (storyboard design specs vs implemented courses)
  - Updated all 6 reviewers with dual modes: STORYBOARD REVIEW vs LIVE CONTENT REVIEW
  - Storyboard mode: Reviews design quality, specifications clarity, feasibility, accessibility planning
  - Live content mode: Tests actual functionality, measures compliance, validates implementation
  - Fixed edge cases: Unbuilt widgets in storyboards = EXPECTED, in live content = CRITICAL
  - Updated report format to clearly indicate review mode and content type
  - Added example invocations showing when to use each mode
  - Prevents confusion: No longer tests browser compatibility on markdown design documents
- **2.4.1** (2025-10-20): Added automatic quality enforcement hooks
  - 4 new hooks: Smart content validator, educational context loader, protected content guardian, storyboard auto-formatter
  - Zero API tokens consumed (pure bash/Python automation)
  - Automatic WCAG/QM/Bloom's validation after every edit (1-3 seconds)
  - Session startup loads educational standards context automatically
  - Protects published content from accidental edits
  - Auto-formats storyboards for platform branding compliance
  - Added .education-toolkit-config.json support for course-specific settings
- **2.4.0** (2025-10-17): Added executable skills for automation
  - 3 new skills: assessment-template-generator, accessibility-audit-tools, qm-validator
  - Python automation for PAIRR templates, AI roleplay configs, diagnostic rubrics
  - Automated WCAG 2.2 AA accessibility testing (color contrast, alt text, headings, ARIA)
  - Quality Matters validation (outcome-criteria alignment, rubric math, measurable language)
  - Agents proactively invoke skills for template generation and validation
  - Dual distribution: bundled with plugin (default) + standalone ZIPs for reusability
  - Saves 10-15 minutes per assessment through automation
  - Added SKILLS-INSTALLATION.md with comprehensive setup guide
- **2.3.0** (2025-10-17): Added peer design review simulation
  - Added peer-review-simulator agent with 6 ID specialists (Emma, Marcus, Priya, James, Sarah, Alex)
  - Added /peer-review slash command
  - Multi-perspective comprehensive review with cross-reviewer theme prioritization
  - 10 agents total, 10 slash commands
- **2.0.0** (2025-10-11): Major update with evidence-based assessment methodologies
  - Added PAIRR (Peer and AI Review + Reflection) methodology to assessment-designer
  - Added AI Roleplay exercise design guidance (diagnostic, formative, summative)
  - Added diagnostic rubrics (3-level) to rubric-generator
  - Added uplimit-storyboard-builder agent (9 agents total, up from 7)
  - Consolidated from claude-subagents NPM package (deprecated)
- **1.0.0** (2025-10-10): Initial release with 7 agents, 6 commands, 464 KB knowledge base
